# -*- coding: utf-8 -*-
"""StockTradingAC.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/nandahkrishna/StockTrading/blob/master/TradingActorCritic.ipynb
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy import signal
import torch
import torch.optim as optim
import torch.nn.functional as F

from ac.environment import Environment
from ac.policy import Policy

torch.manual_seed(20)
torch.set_default_tensor_type(torch.DoubleTensor)

# df = pd.read_csv('aapl.us.txt').iloc[6000:8001]
df = pd.read_csv('data/^GSPC.csv')
apl_open = np.array(df['Open'])
apl_close = np.array(df['Close'])

apl_open = signal.detrend(apl_open)
apl_close = signal.detrend(apl_close)
print(apl_open.min(), apl_close.min())

apl_open += (-apl_open.min() + 1)
apl_close += (-apl_close.min() + 1)

print(apl_open.min(), apl_close.min())

apl_open_orig = np.array(df['Open'])
apl_close_orig = np.array(df['Close'])

env = Environment(open=apl_open, close=apl_close, starting_cash=1000, randomize_cash=100, starting_shares=100,
                  randomize_shares=10, max_stride=4, series_length=499)
model = Policy().cuda()
optimizer = optim.Adam(model.parameters(), lr=4e-4)

env.reset()
del model.rewards[:]
del model.saved_actions[:]

gamma = 0.8
log_interval = 40
running_reward = 0


def finish_episode():
    R = 0
    saved_actions = model.saved_actions
    policy_losses = []
    value_losses = []
    rewards = []
    for r in model.rewards[::-1]:
        R = r + (gamma * R)
        rewards.insert(0, R)
    rewards = torch.tensor(rewards)
    epsilon = (torch.rand(1) / 1e4) - 5e-5
    rewards += epsilon
    
    for (log_prob, value), r in zip(saved_actions, rewards):
        a_reward = torch.tensor(r - value.item()).cuda()
        policy_losses.append(-log_prob * a_reward)
        value_losses.append(F.smooth_l1_loss(value, torch.tensor([r]).cuda()))
        
    optimizer.zero_grad()
    loss = torch.stack(policy_losses).sum() + torch.stack(value_losses).sum()
    loss = torch.clamp(loss, -1e-5, 1e5)
    loss.backward()
    optimizer.step()
    del model.rewards[:]
    del model.saved_actions[:]


for episode in range(0, 4000):
    state = env.reset()
    reward = 0
    done = False
    msg = None
    while not done:
        action = model.act(state)
        state, reward, done, msg = env.step(action)
        model.rewards.append(reward)
        if done:
            break
    running_reward = running_reward * (1 - 1 / log_interval) + reward * 1 / log_interval
    finish_episode()
    
    if msg['msg'] == 'done' and running_reward > 300 and env.portfolio_value() > env.starting_portfolio_value:
        print('Early stop: ', int(reward))
        break
    if episode % log_interval == 0:
        print('Episode {}: start {:.1f}, end {:.1f} because {} @ t={}, last reward {:.1f}, running reward {:.1f}' \
              .format(episode, env.starting_portfolio_value, env.portfolio_value(), msg['msg'],
                      env.cur_timestep, reward, running_reward))

total_rewards = 0
total_profits = 0
failed_goes = 0
num_goes = 50

for j in range(num_goes):
    env.reset()
    reward_this_go = -1e8
    for i in range(0, env.series_length + 1):
        action = model.act(env.state)
        next_state, reward, done, msg = env.step(action)
        if msg['msg'] == 'done':
            reward_this_go = env.portfolio_value()
            break
        if done:
            break
    total_profits += (env.portfolio_value() - env.starting_portfolio_value) / env.starting_portfolio_value
    if reward_this_go == -1e8:
        failed_goes += 1
    else:
        total_rewards += reward_this_go

if failed_goes == num_goes:
    print('Failed all!')
else:
    print('Failed goes: {}/{}, Average reward per successful game: {}'.format(
        failed_goes, num_goes, total_rewards / (num_goes - failed_goes)))
    print('Average profit per game: ', total_profits / num_goes)
    print('Average profit per finished game: ', total_profits / (num_goes - failed_goes))

print('Starting portfolio value ', env.portfolio_value())
for i in range(0, env.series_length + 1):
    action = model.act(env.state)
    next_state, reward, done, msg = env.step(action)
    if msg['msg'] == 'bankrupted self':
        print('Bankrupted self by 1')
        break
    if msg['msg'] == 'sold more than have':
        print('Sold more than have by 1')
        break
    print('{}, have {} stocks and {} cash'.format(msg['msg'], next_state[0], next_state[1]))
    if msg['msg'] == 'done':
        print(next_state, reward)
        print('Total portfolio value ', env.portfolio_value())
        break

env.reset()
complete_game = False

while not complete_game:
    bought_apl_at = []
    sold_apl_at = []
    bought_apl_at_orig = []
    sold_apl_at_orig = []
    nothing_at = []
    b_action_times = []
    s_action_times = []
    n_action_times = []
    starting_val = env.starting_portfolio_value
    print('Starting portfolio value: {}'.format(starting_val))
    for i in range(0, env.series_length + 1):
        action = model.act(env.state)
        if action == 0:
            bought_apl_at.append(apl_open[env.cur_timestep])
            bought_apl_at_orig.append(apl_open_orig[env.cur_timestep])
            b_action_times.append(env.cur_timestep)
        if action == 1:
            sold_apl_at.append(apl_close[env.cur_timestep])
            sold_apl_at_orig.append(apl_close_orig[env.cur_timestep])
            s_action_times.append(env.cur_timestep)
        if action == 2:
            nothing_at.append(0)
            n_action_times.append(env.cur_timestep)
        next_state, reward, done, msg = env.step(action)
        if msg['msg'] == 'bankrupted self':
            env.reset()
            break
        if msg['msg'] == 'sold more than have':
            env.reset()
            break
        if msg['msg'] == 'done':
            print('{}, have {} stocks and {} cash'.format(msg['msg'], next_state[0], next_state[1]))
            val = env.portfolio_value()
            print('Finished portfolio value {}'.format(val))
            complete_game = True
            env.reset()
            break

plt.figure(1, figsize=(14, 5))
plt.plot(range(0, len(apl_open)), apl_open)
plt.plot(b_action_times, bought_apl_at, 'ro')
plt.plot(s_action_times, sold_apl_at, 'go')
plt.plot(n_action_times, nothing_at, 'yx')

plt.figure(1, figsize=(14, 5))
plt.plot(range(0, len(apl_open_orig)), apl_open_orig)
plt.plot(b_action_times, bought_apl_at_orig, 'ro')
plt.plot(s_action_times, sold_apl_at_orig, 'go')
plt.plot(n_action_times, nothing_at, 'yx')